{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ER-190C] Homework 10: Classification Trees\n",
    "----\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "[Introduction](#intro) <br>\n",
    "[1. The Data](#data) <br>\n",
    "[2. Decision Trees From Scratch](#scratch) <br>\n",
    "[3. Implementing with Scikit-learn](#sk) <br>\n",
    "[4. Ensemble Methods](#improve) <br>\n",
    "[5. Comparing Methods](#compare) <br>\n",
    "\n",
    "**Dependencies:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import os.path\n",
    "from shutil import copyfile\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use(\"fivethirtyeight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xlrd\n",
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Introduction <a name = 'intro'> </a>\n",
    "\n",
    "Decision trees are a powerful prediction method and are easy to interpret. A decision tree can explain exactly why a specific prediction was made. They usually aren't used by themselves, but many trees are used for ensemble methods such as random forests and bagging. Trees are, overall, a relatively more accessible method for predictive modeling since they are used for both regression and classification and can take in both continuous and categorical data.\n",
    "\n",
    "In this homework, we'll be doing a brief exploration of the CalEnviroScreen dataset, testing out how to make a tree from scratch, as well as implementing various ensemble methods using scikit-learn. It'll be a comprehensive survey of trees and the multitude of algorithms that arise from one tree!\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "## 1. The Data <a name='data'></a>\n",
    "\n",
    "In this homework, we will be working with the [California Communities Environmental Health Screening Tool (CalEnviroScreen)](https://oehha.ca.gov/calenviroscreen/report/calenviroscreen-30), which uses demographic and environmental information to identify communities that are susceptible to various types of pollution. The various variables in this dataset contribute to the CES score, which reflects a community's environmental conditions and its vulnerability to environmental pollutants.\n",
    "\n",
    "We worked with this dataset in lab and this will be one of two homeworks that utilize this dataset.  Although this dataset scores around 8000 of census tracts in California, we'll focus on a subset which makes the decision tree slightly more interpretable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getfile(filename):\n",
    "    \n",
    "    if os.path.isfile(filename) == False: # check if you've got the file, if not, download it\n",
    "        weborlocal = input('File is not in the present working directory.  Get it from the web, or local? ')\n",
    "        \n",
    "        if weborlocal == 'web':\n",
    "            url = input('What is the url? ')\n",
    "            urllib.request.urlretrieve(url, filename);\n",
    "        \n",
    "        elif weborlocal == 'local':\n",
    "            directory = input('What\\'s the file\\'s directory? ')\n",
    "            print(directory+filename)\n",
    "            \n",
    "            if os.path.isfile(directory+filename)==True:\n",
    "                copyfile(directory+filename, filename)\n",
    "            else:\n",
    "                print('Can\\'t find the file.  Check the directory and make sure the path ends in \"/\".')\n",
    "        else:\n",
    "            print('Please choose \"web\" or \"local\".  Try again.')\n",
    "    \n",
    "    else:\n",
    "        print('That file is in the present working directory.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'ces3results.xlsx'\n",
    "getfile(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've just obtained an excel spreadsheet of environmental data from the California EnviroScreen data set. Now, we will format the spreadsheet into a DataFrame object in order to explore its properties. \n",
    "\n",
    "Documentation on Pandas' excel methods can be found at [here](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_excel.html#pandas.read_excel).\n",
    "\n",
    "First we will focus on the first sheet, which contains the data that we'll work with in this homework; run the following cell to look at the four sheets in this excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl = pd.ExcelFile(filename)\n",
    "print(xl.sheet_names) # display a list of the sheets in the spreadsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Question 1.1:</b> Load the first sheet of the Excel file and assign it to the variable `df0`. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE\n",
    "...\n",
    "df0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we explore this dataset, look at some of the features -- notice that this dataset doesn't include the units of most of it's features. Let's take a look at a different sheet.\n",
    "\n",
    "Run the following cell to load the data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = xl.parse('Data Dictionary').rename(columns = {'CalEnviroScreen 3.0: Data Dictionary':'Variable', \n",
    "                                                   'Unnamed: 1': 'Description',\n",
    "                                                   'Unnamed: 2': 'CES Category'})\n",
    "dd.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on, let's check the sheet we just loaded. \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Question 1.2:</b> Does the number of columns in the first sheet correspond with the number of rows of the variables in the table we just created? Your answer should return a boolean value.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've checked if the features and columns correspond, let's move on and take a closer look at the description of these variables.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Question 1.3:</b> In the `dd` dataframe, drop the rows that contain `Pctl` in the variable name and drop the last column  as well. \n",
    "\n",
    "Make sure the shape is correct and that the CES categories match up with the variable names. You can check that on the second page of [this fact sheet](https://oehha.ca.gov/media/downloads/calenviroscreen/fact-sheet/ces30factsheetfinal.pdf).\n",
    "\n",
    "Note: If you don't pass, rerun the cell that loads this dataframe (two cells before this one)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE\n",
    "...\n",
    "...\n",
    "\n",
    "assert dd.shape == (35, 3)\n",
    "dd.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that not all of the variables have an explicit description of the unit of measurement, or are a scaled or weighted metric. Regardless, we have an idea of the variables we are working with and their units of measurement.\n",
    "\n",
    "Let's move on and take a look at the data for the city of Berkeley.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Question 1.4:</b> Find all instances that Berkeley appears in the dataset and assign it to the variable `berkeley`. Then, select the columns `Census Tract`, `ZIP`, `CES 3.0 Score`, `CES 3.0 Percentile Score`, `Housing Burden`, `Poverty`, `Traffic`, `Groundwater Threats`, `Pollution Burden`, and `Drinking Water`.\n",
    "\n",
    "<br>\n",
    "\n",
    "Assign this resulting table to `berkeley_ft`, and print its head.\n",
    "\n",
    "<br>\n",
    "\n",
    "Note: For the `CES 3.0 Percentile Score` column, the actual name is 'CES 3.0 \\nPercentile Range'\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE\n",
    "berkeley = ...\n",
    "berkeley_ft = ...\n",
    "berkeley_ft.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the columns of the resulting table above. Even though these indicators are somewhat self-explanatory, let's take a closer a look at the indicators, how they were measured, and other aspects that aren't necessarily very clear upon first glance. Take a look at the [CES documentation](https://oehha.ca.gov/media/downloads/calenviroscreen/report/ces3report.pdf) and answer the following question.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Question 1.5:</b> Look through the section on indicator description and analysis. For **three** of the six indicators (or variables) for the columns left of `CES 3.0 Score`, find the following information:<br><br>\n",
    "\n",
    "- How is the indicator measured? <br>\n",
    "- What is the data source? <br>\n",
    "- Why is this indicator used for the CES score (why is is relevant)? <br><br>\n",
    "\n",
    "*Fill in your answers directly into the cell below.* Remember: you only need to do three of the six total.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Housing Burden**  <br>\n",
    "Measurement:        <br>\n",
    "Data source:        <br>\n",
    "Relevance:\n",
    "\n",
    "**Poverty**         <br>\n",
    "Measurement:        <br>\n",
    "Data source:        <br>\n",
    "Relevance:\n",
    "\n",
    "**Traffic**        <br>\n",
    "Measurement:       <br>\n",
    "Data source:       <br>\n",
    "Relevance:\n",
    "\n",
    "**Groundwater Threats** <br>\n",
    "Measurement:            <br>\n",
    "Data source:            <br>\n",
    "Relevance:\n",
    "\n",
    "**Pollution Burden** <br>\n",
    "Measurement:         <br>\n",
    "Data source:         <br>\n",
    "Relevance:\n",
    "\n",
    "**Drinking Water** <br>\n",
    "Measurement:         <br>\n",
    "Data source:         <br>\n",
    "Relevance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We also dropped the percentiles of all of the indicators in our data dictionary. Because the percentiles show relative scores of the indicators, we lose the original measurements. The percentiles are used in the final calculation of the CES score, in which the percentiles of the indicators in the four groups are averaged. The formula for the CES score is the shown below.\n",
    "\n",
    "<img src=\"ces_calc.png\" height=25 width=400>\n",
    "\n",
    "Let's compare the two rows in our table, specifically the rows with Census Tract `6001421100` and `06001422000`. Run the following cell to load the two rows.\n",
    "\n",
    "<a name='berkeleytable'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind1 = berkeley_ft[berkeley_ft['Census Tract'] == 6001421100.0]\n",
    "ind2 = berkeley_ft[berkeley_ft['Census Tract'] == 6001422000.0]\n",
    "ind1.append(ind2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two communities are extremely different -- one has an extremely low score and one has a high score, and the values in the features are also pretty varied. Let's take a look at where their scores lie in various histograms of the features.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Question 1.6:</b> Plot a distribution for the `Poverty` variable of all Berkeley tracts as well as for `Pollution Burden`. Along with these histograms, plot the points in which these two tracts lie within the distribution (hint: use `plt.scatter` for this).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE ('Poverty' histogram)\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE ('Pollution Burden' histogram)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's take a look at the demographic profiles of these two communities. This data is stored in the `Demographic profile` sheet.\n",
    "\n",
    "<div class = \"alert alert-warning\">\n",
    "\n",
    "<b>Question 1.7:</b> Similar to what we did earlier by comparing the two rows, compare the two tracts' demographic information (i.e. display the two rows together). We've loaded the sheet into a dataframe `dp` and have renamed the columns for you.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = xl.parse('Demographic profile')\n",
    "dp = dp.rename(columns = {'Census Tract ': 'Census Tract',\n",
    "                          'Age group from 2010 Census (%)':'Children < 10 (%)', \n",
    "                          'Unnamed: 7':'Pop 11-64 years (%)',\n",
    "                          'Unnamed: 8':'Elderly > 65 (%)', \n",
    "                          'Race or ethnicity from 2010 Census (%)':'Hispanic (%)',\n",
    "                          'Unnamed: 10':'White (%)',\n",
    "                          'Unnamed: 11':'African American (%)', \n",
    "                          'Unnamed: 12': 'Native American (%)', \n",
    "                          'Unnamed: 13':'Asian American (%)', \n",
    "                          'Unnamed: 14': 'Other (%)'})\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "demo1 = ...\n",
    "demo2 = ...\n",
    "demo1.append(demo2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're curious, you can take a look at where these census tracts are located by inputting the coordinates in [maps](https://www.google.com/maps).\n",
    "- Coordinates for the first row: (+37.8994340, -122.2661928)\n",
    "- Coordinates for second row: (+37.8590327, -122.3013426)\n",
    "\n",
    "You can then find the tract areas by using this [web tool](https://data.cityofberkeley.info/Demographics/Census-Tract-Polygons-2010/peq3-2arw) provided by the city of Berkeley!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## 2. A Decision Tree From Scratch <a name = 'scratch'></a>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating a decision tree from scratch! Even though trees are pretty easy to interpret, there's a lot that runs to create the final tree. We'll walk through creating a tree which will hopefully help our intuition when we use scikit-learn later in this homework.\n",
    "\n",
    "First, let's take a very small subset of our data so that our tree will be easy to work with. There are a few things we're doing here to make this process more digestible -- we're taking a sample of 10 from the table and we are classifying the top and bottom groups of the CES dataset.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Question 2.1:</b> In the following cell, we've loaded the first sheet of the excel file into a dataframe called `data`. \n",
    "\n",
    "<br>\n",
    "\n",
    "First, find the indices of the groups with the highest (96-100th percentiles) and lowest (1-5th percentiles) scores. You should have two arrays, and assign them to the variables `top` and `bottom` respectively. \n",
    "\n",
    "<br>\n",
    "\n",
    "Then, fill out the ellipses in the `sample` dataframe method -- take a sample of 10, and set the random_state to 1.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE\n",
    "\n",
    "data = data = df0.rename(columns={'CES 3.0 \\nPercentile Range':'CES 3.0 Percentile Range'})\n",
    "\n",
    "...\n",
    "\n",
    "scratch = data.iloc[top.append(bottom)].dropna()\n",
    "scratch = scratch[[\"Groundwater Threats\", \n",
    "                   \"Drinking Water\",           \n",
    "                   \"CES 3.0 Percentile Range\"]].sample(...)\n",
    "scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we want to do when we building a tree is how can we create each split in the tree. To do so, we'll start with calculating the gini index. \n",
    "\n",
    "The gini index (also called gini coefficient) is the cost function that we use to evaluate splits in our dataset. It gives us an idea of the quality of a split by how \"pure\" it is, or how mixed the two classes are after the split. The value ranges from 0 to .5. For a two class problem, 0 denotes a perfect separation while 0.5 is the worst split, (a split that results in 50/50 classes in each group result).\n",
    "\n",
    "The index is calculated as follows:\n",
    "\n",
    "$$ G = \\sum_{k = 1}^{K}{\\hat{p}_{mk}(1 - \\hat{p}_{mk})} =  \\sum_{k = 1}^{K} (\\hat{p}_{mk}- \\hat{p}^2_{mk})$$\n",
    "\n",
    "where $G$ is the Gini index, and $\\hat{p}_{mk}$ is the fraction of training data belonging to each class ($k$) in given region ($m$).\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Question 2.2:</b> Take a look at the following cell, which contains the code in order to calculate the gini index. There are letters following three pound symbols. Write down what the line below is doing for each letter.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_index(groups, classes):\n",
    "    ### A ###\n",
    "    n_instances = float(sum([len(group) for group in groups]))\n",
    "    ### B ###\n",
    "    gini = 0.0\n",
    "    for group in groups:\n",
    "        size = float(len(group))\n",
    "        if size == 0:\n",
    "            continue\n",
    "        score = 0.0\n",
    "        ### C ###\n",
    "        for class_val in classes:\n",
    "            p = [row[-1] for row in group].count(class_val) / size\n",
    "            score += p - p * p\n",
    "        ### D ###\n",
    "        gini += (score)\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***YOUR ANSWERS HERE***\n",
    "\n",
    "**A)**\n",
    "\n",
    "**B)**\n",
    "\n",
    "**C)**\n",
    "\n",
    "**D)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "Now, we can look at creating a split. A split is comprised of an attribute in the dataset and a value and creating one involves three parts.\n",
    "\n",
    "1. Calculating the gini index\n",
    "2. Splitting a dataset\n",
    "3. Evaluating all splits\n",
    "\n",
    "We'll look at the second step now!\n",
    "\n",
    "Splitting a dataset means separating a dataset into two lists of rows given the index of an attribute and a split value for that attribute. Once we have the two groups, we can use the gini index above to evaluate the cost of the split. In this procedure, we iterate over each row to check if the attribute value is below or above the split value and then we'll assign it to the left or right group respectively.\n",
    "\n",
    "Run the following cell to load the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_split(index, value, dataset):\n",
    "    left, right = list(), list()\n",
    "    for row in dataset:\n",
    "        if row[index] < value:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return left, right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Question 2.3:</b> What does the group `right` in this function represent?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***YOUR ANSWER HERE***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "We can create our splits now. Given a dataset, we must check every value on each attribute as a candidate split, evaluate the cost of the split and find the best possible split we could make. The lowest gini score across all the features would then be chosen as the best split and a node would be created.\n",
    "\n",
    "We will use a dictionary to represent a node in the decision tree as we can store data by name. When selecting the best split and using it as a new node for the tree we will store the index of the chosen attribute, the value of that attribute by which to split and the two groups of data split by the chosen split point.\n",
    "\n",
    "The function below runs this procedure. Run the cell to load function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(dataset):\n",
    "    class_values = list(set(row[-1] for row in dataset))\n",
    "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "    for index in range(len(dataset[0])-1):\n",
    "        for row in dataset:\n",
    "            groups = test_split(index, row[index], dataset)\n",
    "            gini = gini_index(groups, class_values)\n",
    "            if gini < b_score:\n",
    "                b_index = index\n",
    "                b_value = row[index]\n",
    "                b_score = gini\n",
    "                b_groups = groups\n",
    "    return {'index':b_index, 'value':b_value, 'groups':b_groups}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all of the tools to find the best splits of the tree -- let's see how we can use them to *build* one from the ground up (or from the top since trees are inverted...). \n",
    "\n",
    "Building a tree takes two main steps: finding leaves (e.g. when to stop the tree) and recursively splitting the tree.\n",
    "\n",
    "The following cell contains the function that returns the most common output in a list of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a terminal node value\n",
    "def to_terminal(group):\n",
    "    outcomes = [row[-1] for row in group]\n",
    "    return max(set(outcomes), key=outcomes.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Question 2.4:</b> What is a problem when fitting a tree too deeply? What about having too many nodes?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***YOUR ANSWER HERE***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "We're almost there! The following cell contains the function that performs the recursive splitting. \n",
    "\n",
    "Here are the steps of this procedure:\n",
    "\n",
    "1. Two groups of data split by the node are extracted and deleted from the node. The node no longer requires access to these data when we work on the node.\n",
    "2. We check if either left or right group of rows is empty and if so we create a terminal node using the data and computed scores we have.\n",
    "3. Check if we have reached our maximum depth. If we have, create a terminal node.\n",
    "4. If the group of rows is too small, we process the left child and create a terminal node. Otherwise, we create and add the left node in a depth-first fashion until the bottom of the tree is reached on this branch.\n",
    "5. Right side is then processed in the same manner, as we rise back up the constructed tree to the root.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Question 2.5:</b> Similar to question 2.2, look at the following cell and note what each control case is doing in the `split` function.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(node, max_depth, min_size, depth):\n",
    "    left, right = node['groups']\n",
    "    del(node['groups'])\n",
    "    ### A ###\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = to_terminal(left + right)\n",
    "        return\n",
    "    ### B ###\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "        return\n",
    "    ### C ###\n",
    "    if len(left) <= min_size:\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        node['left'] = get_split(left)\n",
    "        split(node['left'], max_depth, min_size, depth+1)\n",
    "    ### D ###\n",
    "    if len(right) <= min_size:\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        node['right'] = get_split(right)\n",
    "        split(node['right'], max_depth, min_size, depth+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***YOUR ANSWER HERE***\n",
    "\n",
    "**A)**\n",
    "\n",
    "**B)**\n",
    "\n",
    "**C)**\n",
    "\n",
    "**D)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Awesome! We're ready to build our tree. In the following cell, we have our `build_tree` and `print_tree` function. Most of the steps were abstracted away in `get_split` and `split` functions. Before we use the function, we can't directly use a pandas dataframe or series as input to this function.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Question 2.6:</b> In the cell below, assign a list of the rows of the `scratch` dataframe to the variable `rows`. Run the cell and see the resulting tree. What do the inqualities represent? How does it relate to the data we inputted into the tree?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(train, max_depth, min_size):\n",
    "    root = get_split(train)\n",
    "    split(root, max_depth, min_size, 1)\n",
    "    return root\n",
    "\n",
    "def print_tree(node, depth=0):\n",
    "    if isinstance(node, dict):\n",
    "        print('%s[X%d < %.3f]' % ((depth*' ', (node['index']+1), node['value'])))\n",
    "        print_tree(node['left'], depth+1)\n",
    "        print_tree(node['right'], depth+1)\n",
    "    else:\n",
    "        print('%s[%s]' % ((depth*' ', node)))\n",
    "\n",
    "# YOUR CODE HERE   \n",
    "\n",
    "rows = ...\n",
    "scratch_tree = build_tree(rows, 1, 1)\n",
    "print_tree(scratch_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***YOUR TEXT ANSWER HERE*** (Describe what the inquality represents and how it relates to the data we inputted into the tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Just for fun, we can also use this tree for prediction! Let's use the observation that had the lower score from [earlier](#berkeleytable) in the table of the two Berkeley census tracts we chose. Run the following cell to see the expected and a predicted value from the observation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(node, row):\n",
    "    if row[node['index']] < node['value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return predict(node['left'], row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return predict(node['right'], row)\n",
    "        else:\n",
    "            return node['right']\n",
    "\n",
    "row = ind1[['Groundwater Threats', 'Drinking Water', 'CES 3.0 \\nPercentile Range']].values.tolist()[0]\n",
    "prediction = predict(scratch_tree, row)\n",
    "print('Expected: ', ind1['CES 3.0 \\nPercentile Range'].values[0])\n",
    "print('Got: ', prediction, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the expected percentile range! Now that we have a better idea of how a tree works under the hood, let's move on to scikit-learn and create a more complex tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Implementing with Scikit-Learn <a name = 'sk'></a>\n",
    "\n",
    "Since our data has a bunch of features, creating and tuning a tree from scratch would be a hassle and would not necessarily lead to optimal results. Here's where scikit-learn comes in and streamlines the process for us.\n",
    "\n",
    "We'll be using scikit-learn's `DecisionTreeClassifier` ([documenation](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)) in this section and use a larger subset of the data to create a more complex model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn import tree\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### 3.1. The First Tree\n",
    "\n",
    "For our first model, we'll use the features to predict which score bracket a certain tract lies in, namely we want to use these features to predict the `CES 3.0 Percentile Range`. For the sake of interpretability, we'll focus on the 91th percentile and above, and up to the 10th percentile of scores.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Question 3.1.1:</b> Before we move on, what is a problem that can stem from using such a specific subset of our data to build a classifier?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Your Answer Here***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the scores that have been binned already in the dataset so we know how to set up our model. Run the following cell to see them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['CES 3.0 Percentile Range'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have a total of four values that our tree can potentially classify if we want to take the top and bottom 10% of scores.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Question 3.1.2:</b> Get the index values for the data in the `96-100% (highest scores)` and `91-95%` range, and assign the values to the variable `pct90`. Do the same for `6-10%` and `1-5% (lowest scores)`, assigning the resulting values to the variable `pct10`.\n",
    "\n",
    "<br>\n",
    "\n",
    "Then, use `pct90` and `pct10` to get the resulting table and reassign it to `data`. This table should only contain the four ranges in the column `CES 3.0 Percentile Range`\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE\n",
    "\n",
    "...\n",
    "print(pct90)\n",
    "\n",
    "...\n",
    "...\n",
    "...\n",
    "print(pct10)\n",
    "\n",
    "data = ...\n",
    "\n",
    "assert len(data['CES 3.0 Percentile Range'].unique()) == 4\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our desired observations, let's filter through the certain features we want to use in our classification tree. Remember that a tree can use both continuous and categorical data, but we probably don't want to work with features like the county or zip code since it would clutter our data if we wanted to encode them. Run the following cell to drop the columns as well as the NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns = ['Census Tract', 'CES 3.0 Score', ' CES 3.0 Percentile', \n",
    "                            'California County', 'ZIP', 'City', 'Longitude', 'Latitude'])\n",
    "data = data.dropna()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we need to do before we create our decision tree is to drop the `Pctl` columns, as well as create the testing, training, and validation sets.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Question 3.1.3:</b> Drop all columns that contain the string `Pctl` for our set of features. Don't forget to drop the percentile range column. We don't want that in our features! \n",
    "\n",
    "Assign the resulting table the variable `features` and assign the percentile ranges to the variable `target`.\n",
    "\n",
    "Lastly, create the training, testing, and validation sets by splitting out a testing set first, then split the remaining (non-testing) data into a training set and validation set. Use `random_state = 1` both times, and create an 80/20 split to get the test data, and a 75/25 split on the remaining data to get the train/validation split.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE\n",
    "\n",
    "drop_columns = ...\n",
    "...\n",
    "\n",
    "features = ...\n",
    "target = ...\n",
    "\n",
    "# split test set\n",
    "X, X_test, y, y_test = train_test_split(...)\n",
    "\n",
    "# split between train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We can finally create a tree! \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Question 3.1.3:</b> Instantiate a `DecisionTreeClassifer` model and call it `first_tree`. Fit the model using the training data, and score it using both the training and validation set. Assign the scores to the variables `train_score` and `val_score` respectively.\n",
    "    \n",
    "Hint: use the `.score` method on the `first_tree` model to score it.  \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE\n",
    "...\n",
    "...\n",
    "\n",
    "print(\"Number of features: {}\".format(first_tree.tree_.n_features))\n",
    "print(\"Number of nodes (leaves): {}\".format(first_tree.tree_.node_count), \"\\n\")\n",
    "\n",
    "...\n",
    "...\n",
    "\n",
    "print('Train Score: ', train_score)\n",
    "print('Validation Score: ', val_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score isn't too shabby for a tree that was put together pretty quickly. The nice part about decision trees is that they're easy to visualize and interpret and with scikit-learn, we can export the an image of the tree. Unfortunately, due limitations on DataHub, we can't output an image directly in a notebook. \n",
    "\n",
    "Luckily, we can copy the code and visualize the tree on [Webgraphviz](http://webgraphviz.com). By running the following cell, you'll see a pretty long output -- follow the link and copy and paste the output to get a visualization of the decision tree we fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import graphviz\n",
    "print(tree.export_graphviz(first_tree, feature_names=X.columns, out_file=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Question 3.1.4:</b> What does each line in the box (the boxes that are displayed through Webgraphviz) mean?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***YOUR ANSWER HERE*** <br>\n",
    "First line:\n",
    "\n",
    "Second line:\n",
    "\n",
    "Third line:\n",
    "\n",
    "Fourth Line:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Question 3.1.4:</b>   Based on the visualization, can you see any problems or pitfalls with the model we just created? Can you infer the \"most and least important\" features?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***YOUR ANSWER HERE***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<a name='first'></a>\n",
    "With scikit-learn, we're also able to check the feature importances. Running the following cell, we can see the features and the their importance based on the data we used to fit the tree -- this can be helpful when tuning or pruning the tree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'Feature': X.columns, 'Importance': first_tree.feature_importances_})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## 4. Ensemble Methods <a name = 'improve'></a>\n",
    "\n",
    "In the previous section, we mainly were focusing on fitting one tree and editing it's hyperparameters or pruning it. Otherwise, we do anything more to alter it. Also, you may have noticed that, although the training scores were  extremely high, the validation scores usually weren't as high. This is where ensemble methods come in, namely bagging, random forests, and boosting. We call them \"ensemble methods\" because we fit a many trees and, for example in bagging, we take the average of all of the models as a means to reduce the problem of overfitting.\n",
    "\n",
    "----\n",
    "\n",
    "### 4.1. Bagging\n",
    "\n",
    "Bagging takes in $B$ different bootstrapped training data sets and we train our tree on each $b$th training set to get $\\hat{f}^{*b}(x)$. Then, we average all of the predictions to get \n",
    "\n",
    "$$\\hat{f}_{bag}(x) = \\frac{1}{B} \\sum_{b=1}^{B}\\hat{f}^{*b}(x)$$\n",
    "\n",
    "In terms of classifying, bagging records the class from each bootstrapped sample and chooses the most commonly occurring majority class among the B predictions.\n",
    "\n",
    "Let's proceed by using `BaggingClassifier` from scikit-learn's ensemble module ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)). \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Question 4.1.1:</b> Use the training data and fit the bagging classifier. Score the model -- how much has the score improved?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "...\n",
    "...\n",
    "\n",
    "bag_train_score = ...\n",
    "bag_val_score = ...\n",
    "\n",
    "print('Train Score: ', bag_train_score)\n",
    "print('Validation Score: ', bag_val_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before, we can use the randomized parameter search to find optimal parameters for our model.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Question 4.1.2:</b> Create a dictionary for the parameter distribution and fit a `RandomizedSearchCV` with `cv = 10` and `n_iter = 50`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_dist = {}\n",
    "\n",
    "rnd_search = RandomizedSearchCV(...)\n",
    "\n",
    "...\n",
    "\n",
    "print(rnd_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Question 4.1.2:</b> Set the parameters in `bag_tree` and re-score the model.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE\n",
    "\n",
    "...\n",
    "\n",
    "bag_train_score = ...\n",
    "bag_val_score = ...\n",
    "\n",
    "print('Train Score: ', bag_train_score)\n",
    "print('Validation Score: ', bag_val_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that bagging the model has a better score than the first model we created. Even if we get a better model in the end, there are still some consequences.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Question 4.1.3:</b> What are some drawbacks of bagging compared to a single decision tree? Is there a way to compare the quality of a single decision tree model compared to a bagged model?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***YOUR ANSWER HERE***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### 4.2. Random Forests\n",
    "\n",
    "Random forests take a similar approach to bagging, in which we fit various decision trees on resampled data. But, when each tree is constructed, not every feature is considered as split candidates for each decision point; we only take a subset of the total predictors in the model.\n",
    "\n",
    "When building a random forest compared to a single decision tree, adding randomization into the features that create the model, then averaging the predictions across models will typically produce a model \"decorrelates\" the trees and in turn is more reliable for prediction.\n",
    "\n",
    "We'll use scikit-learn's `RandomForestClassifier()` to implement our model. The documentation can be found [here](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Question 4.2.1: </b>  Create a default RandomForestClassifier and same as before, fit the training data and score the model.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "...\n",
    "...\n",
    "\n",
    "rf_train_score = ...\n",
    "rf_val_score = ...\n",
    "\n",
    "print('Train Score: ', rf_train_score)\n",
    "print('Validation Score: ', rf_val_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Question 4.2.2: </b>  We'll use RandomizedSearchCV again to find values for our hyperparameters. Look at the documentation to see which parameters we can find using the random search. Then, find the cross-validated parameters and print them. \n",
    "\n",
    "Note: This might take some time, so feel free to move onto the next question while it's working.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE\n",
    "\n",
    "param_dist = ...\n",
    "\n",
    "...\n",
    "\n",
    "...\n",
    "\n",
    "print(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we can choose the number of features or predictors that the random forest will consider in each tree. \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Question 4.2.3: </b>  What if the number of predictors we set is equal to the total number predictors in our regular model? Which method would it be equivalent to?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***YOUR ANSWER HERE***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Question 4.2.4: </b> Once the search has completed and you've printed the parameters, set them in the `rf_tree` model and re-fit the model, and print out the scores.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE\n",
    "...\n",
    "\n",
    "rf_train_score = ...\n",
    "rf_val_score = ...\n",
    "\n",
    "print('Train Score: ', rf_train_score)\n",
    "print('Validation Score: ', rf_val_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to bagging, we cannot visualize a random forest; with a reduction in variance comes at a price of interpretability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### 4.3. Boosting\n",
    "\n",
    "Lastly, we'll cover boosting, which is yet another approach to improve a decision tree model. The boosting approach grows a tree slowly. Each boosting algorithm has a slightly different approach, but the general idea is the same behind each one.\n",
    "\n",
    "First, we'll briefly cover AdaBoost (for ADAptive BOOSTing). The adaptive part of the algorithm comes from how it updates the data for each weak model in the sequence; it combines many relatively weak and inaccurate classifiers. So it shines when a regular classifier doesn't perform well with a given dataset.\n",
    "\n",
    "We can use `AdaBoostClassifier` from scikit-learn to try this out. Run the following cell to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_tree = AdaBoostClassifier()\n",
    "ada_tree.fit(X_train, y_train)\n",
    "\n",
    "ada_train_score = ada_tree.score(X_train, y_train)\n",
    "ada_val_score = ada_tree.score(X_val, y_val)\n",
    "\n",
    "print('Train Score: ', ada_train_score)\n",
    "print('Validation Score: ', ada_val_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops. It doesn't look too great. One thing with AdaBoost is that it doesn't handle noisy data or outliers well. In this case, our data is basically polarized -- we're taking values from the top and bottom 10% -- which means it might not work as well as we want it to.\n",
    "\n",
    "We can turn to gradient boosting, which is another boosting method. Gradient boosting involves three elements:\n",
    "1. A loss function to be optimized.\n",
    "2. A weak learner to make predictions.\n",
    "3. An additive model in which we add weak learners to minimize the loss function.\n",
    "\n",
    "We'll use scikit-learn's `GradientBoostingClassifier` ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)) to see how it performs with our data.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Question 4.3.1: </b> We'll do this the last time! Fit the training data to the GradientBoostingClassifier and score the model. How much did it improve over the original model?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "...\n",
    "...\n",
    "\n",
    "gb_train_score = ...\n",
    "gb_val_score = ...\n",
    "\n",
    "print('Train Score: ', gb_train_score)\n",
    "print('Validation Score: ', gb_val_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also improve the model by tuning the hyperparameters. Since we've done this quite a bit, we've coded the cell for running the search -- just run the following cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {'learning_rate': randint(1, 5),\n",
    "              'max_leaf_nodes': randint(3, 100),\n",
    "              'max_features': ['auto'],\n",
    "              'max_depth': randint(1, 10),\n",
    "              'min_samples_leaf': randint(1, 30)}\n",
    "\n",
    "rnd_gb_search = RandomizedSearchCV(gb_tree, param_distributions=param_dist, \n",
    "                                cv=10, n_iter=50)\n",
    "\n",
    "rnd_gb_search.fit(X_train, y_train)\n",
    "\n",
    "print(rnd_gb_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Question 4.3.2: </b> Once the search is complete, set the parameters and re-fit and score the model.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE\n",
    "\n",
    "...\n",
    "...\n",
    "\n",
    "gb_train_score = ...\n",
    "gb_val_score = ...\n",
    "\n",
    "print('Train Score: ', gb_train_score)\n",
    "print('Validation Score: ', gb_val_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With boosting, we can also find the relative feature importance with this boosted model. This can help with the feature importance like before. \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Question 4.3.3: </b>  Get the feature importances and calculate the *relative* feature importance, which is each feature divided by the maximum feature multiplied by 100. Load the data into a table.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Question 4.3.4: </b> Let's take a look at how these features compare with each other. Sort the values in the dataframe and create a bar chart that displays the feature importances in descending order.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Question 4.3.5: </b> Lastly, compare these values with the values we computed the [very first time](#first). Are these values similar? If not, which one does a better job indicating which features are more important?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***YOUR ANSWER HERE***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "We've gone through many methods of creating a decision tree and tuning and improving it, as well as various algorithms that use multiple trees to create a more reliable tree for prediction. Even though we've primarily have been testing our models with the training and validation set, we would still need to cross validate each of these models to see which one is the optimal one to choose given our data and how the model performs (this is even more important when a lot of the scores turned up around 92-93%). \n",
    "\n",
    "Before you finish up this homework, run the following cell to see which has the highest score with our test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [first_tree, tuned_tree, pruned_tree, bag_tree, rf_tree, ada_tree, gb_tree]\n",
    "for i in models:\n",
    "    print('Test Score: ', i.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Submission\n",
    "\n",
    "Congrats, you're done with homework 10! \n",
    "\n",
    "In order to turn in this assignment, go to the toolbar and click **File** -> **Download as** -> **.html** and **.ipynb**. Submit the files through bCourses.\n",
    "\n",
    "----\n",
    "\n",
    "Notebook developed by: Jason Jiang\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
